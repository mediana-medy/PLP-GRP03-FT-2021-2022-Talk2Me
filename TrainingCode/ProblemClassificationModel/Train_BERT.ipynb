{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTIhnrAoOnqK"
      },
      "source": [
        "# 6 - Transformers for Sentiment Analysis\n",
        "\n",
        "In this notebook we will be using the transformer model, first introduced in [this](https://arxiv.org/abs/1706.03762) paper. Specifically, we will be using the BERT (Bidirectional Encoder Representations from Transformers) model from [this](https://arxiv.org/abs/1810.04805) paper. \n",
        "\n",
        "Transformer models are considerably larger than anything else covered in these tutorials. As such we are going to use the [transformers library](https://github.com/huggingface/transformers) to get pre-trained transformers and use them as our embedding layers. We will freeze (not train) the transformer and only train the remainder of the model which learns from the representations produced by the transformer. In this case we will be using a multi-layer bi-directional GRU, however any model can learn from these representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipmljUMbOnqN"
      },
      "source": [
        "## Preparing Data\n",
        "\n",
        "First, as always, let's set the random seeds for deterministic results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLMec5S4OnqN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40YIX2RMICLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33948b7-abcf-4b17-c2e4-7bfe3733908d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.7/dist-packages (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (6.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.21.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.44.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (2.1.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.21.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.0)\n",
            "Requirement already satisfied: plot_keras_history in /usr/local/lib/python3.7/dist-packages (1.1.30)\n",
            "Requirement already satisfied: sanitize-ml-labels>=1.0.28 in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.0.30)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (3.2.2)\n",
            "Requirement already satisfied: compress-json in /usr/local/lib/python3.7/dist-packages (from sanitize-ml-labels>=1.0.28->plot_keras_history) (1.0.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (1.21.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->plot_keras_history) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->plot_keras_history) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->plot_keras_history) (2018.9)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: plot_keras_history in /usr/local/lib/python3.7/dist-packages (1.1.30)\n",
            "Requirement already satisfied: sanitize-ml-labels>=1.0.28 in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.0.30)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.4.1)\n",
            "Requirement already satisfied: compress-json in /usr/local/lib/python3.7/dist-packages (from sanitize-ml-labels>=1.0.28->plot_keras_history) (1.0.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->plot_keras_history) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->plot_keras_history) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->plot_keras_history) (2018.9)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras==2.3.1\n",
        "!pip install tensorflow==2.1.0\n",
        "!pip install plot_keras_history\n",
        "!pip install h5py==2.10.0\n",
        "!pip install plot_keras_history\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x__i05v8NgqR"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OzlTSEZFOj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5facd344-2763-4647-9731-d76503ccae54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Jul 15 21:39:26 2020\n",
        "\n",
        "@author: isswan\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D,Reshape, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate, LSTM, Bidirectional\n",
        "from keras.models import load_model\n",
        "from keras.models import Model\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn import metrics \n",
        "\n",
        "import joblib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from plot_keras_history import plot_history\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import pickle\n",
        "\n",
        "import transformers\n",
        "\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNEqdGOZlmkO"
      },
      "source": [
        "# ESCONV Classifier - Categories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tm05C0WYcU3"
      },
      "source": [
        "## Data - ESConv.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9SkstJS7Xlx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "9b787123-7903-446b-b795-9bbd762f0a51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       experience_type emotion_type           problem_type  \\\n",
              "0  Previous Experience      anxiety             job crisis   \n",
              "1   Current Experience        anger  problems with friends   \n",
              "2   Current Experience         fear             job crisis   \n",
              "3   Current Experience   depression     ongoing depression   \n",
              "4   Current Experience   depression   breakup with partner   \n",
              "\n",
              "                                           situation  \\\n",
              "0  I hate my job but I am scared to quit and seek...   \n",
              "1  I have complete unsupportive friends its to th...   \n",
              "2  I have been out of work for five weeks in quar...   \n",
              "3           I am depressed staying home due to COVID   \n",
              "4  I found out that my boyfriend had been lying t...   \n",
              "\n",
              "                                        survey_score  \\\n",
              "0  {'seeker': {'initial_emotion_intensity': '5', ...   \n",
              "1  {'seeker': {'initial_emotion_intensity': '5', ...   \n",
              "2  {'seeker': {'initial_emotion_intensity': '4', ...   \n",
              "3  {'seeker': {'initial_emotion_intensity': '4', ...   \n",
              "4  {'seeker': {'initial_emotion_intensity': '5', ...   \n",
              "\n",
              "                                              dialog  \\\n",
              "0  [{'speaker': 'seeker', 'annotation': {}, 'cont...   \n",
              "1  [{'speaker': 'supporter', 'annotation': {'stra...   \n",
              "2  [{'speaker': 'supporter', 'annotation': {'stra...   \n",
              "3  [{'speaker': 'supporter', 'annotation': {'stra...   \n",
              "4  [{'speaker': 'supporter', 'annotation': {'stra...   \n",
              "\n",
              "              seeker_question1                           seeker_question2  \\\n",
              "0  Partner was very supportive  More guidance in conversation or examples   \n",
              "1                                                                           \n",
              "2                           no                                        no    \n",
              "3                           No                                         No   \n",
              "4                Good exercise                                         no   \n",
              "\n",
              "  supporter_question1                               supporter_question2  \n",
              "0                                                                        \n",
              "1       It was simple  The middle screen hover function gets in the way  \n",
              "2                                                                        \n",
              "3                                                                        \n",
              "4                                                                        "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ed64d89-57dd-42f0-89f5-3bc9715940ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experience_type</th>\n",
              "      <th>emotion_type</th>\n",
              "      <th>problem_type</th>\n",
              "      <th>situation</th>\n",
              "      <th>survey_score</th>\n",
              "      <th>dialog</th>\n",
              "      <th>seeker_question1</th>\n",
              "      <th>seeker_question2</th>\n",
              "      <th>supporter_question1</th>\n",
              "      <th>supporter_question2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Previous Experience</td>\n",
              "      <td>anxiety</td>\n",
              "      <td>job crisis</td>\n",
              "      <td>I hate my job but I am scared to quit and seek...</td>\n",
              "      <td>{'seeker': {'initial_emotion_intensity': '5', ...</td>\n",
              "      <td>[{'speaker': 'seeker', 'annotation': {}, 'cont...</td>\n",
              "      <td>Partner was very supportive</td>\n",
              "      <td>More guidance in conversation or examples</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Current Experience</td>\n",
              "      <td>anger</td>\n",
              "      <td>problems with friends</td>\n",
              "      <td>I have complete unsupportive friends its to th...</td>\n",
              "      <td>{'seeker': {'initial_emotion_intensity': '5', ...</td>\n",
              "      <td>[{'speaker': 'supporter', 'annotation': {'stra...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>It was simple</td>\n",
              "      <td>The middle screen hover function gets in the way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Current Experience</td>\n",
              "      <td>fear</td>\n",
              "      <td>job crisis</td>\n",
              "      <td>I have been out of work for five weeks in quar...</td>\n",
              "      <td>{'seeker': {'initial_emotion_intensity': '4', ...</td>\n",
              "      <td>[{'speaker': 'supporter', 'annotation': {'stra...</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Current Experience</td>\n",
              "      <td>depression</td>\n",
              "      <td>ongoing depression</td>\n",
              "      <td>I am depressed staying home due to COVID</td>\n",
              "      <td>{'seeker': {'initial_emotion_intensity': '4', ...</td>\n",
              "      <td>[{'speaker': 'supporter', 'annotation': {'stra...</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Current Experience</td>\n",
              "      <td>depression</td>\n",
              "      <td>breakup with partner</td>\n",
              "      <td>I found out that my boyfriend had been lying t...</td>\n",
              "      <td>{'seeker': {'initial_emotion_intensity': '5', ...</td>\n",
              "      <td>[{'speaker': 'supporter', 'annotation': {'stra...</td>\n",
              "      <td>Good exercise</td>\n",
              "      <td>no</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ed64d89-57dd-42f0-89f5-3bc9715940ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ed64d89-57dd-42f0-89f5-3bc9715940ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ed64d89-57dd-42f0-89f5-3bc9715940ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data = pd.read_json(\"ESConv.json\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ_0exSN7eAp"
      },
      "source": [
        "## Label for Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZexJAMXlzxRA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "df8c6af9-513c-4c4f-f20f-e533d7428a29"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "emotional       363\n",
              "work            280\n",
              "relationship    239\n",
              "friendship      179\n",
              "school          158\n",
              "others           53\n",
              "family           28\n",
              "Name: problem_type, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'emotional',\n",
              " 'family',\n",
              " 'friendship',\n",
              " 'others',\n",
              " 'relationship',\n",
              " 'school',\n",
              " 'work'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# dict(data.loc[data['problem_type']=='academic pressure', 'problem_type']).keys()\n",
        "\n",
        "data.loc[data['problem_type']=='School Bullying', 'problem_type'] = \"school\" #965, 1003\n",
        "data.loc[data['problem_type']=='academic pressure', 'problem_type'] = \"school\"\n",
        "\n",
        "data.loc[data['problem_type']=='problems with friends', 'problem_type'] = \"friendship\"\n",
        "\n",
        "data.loc[data['problem_type']=='job crisis', 'problem_type'] = \"work\"\n",
        "\n",
        "data.loc[data['problem_type']=='breakup with partner', 'problem_type'] = \"relationship\"\n",
        "\n",
        "data.loc[data['problem_type']=='conflict with parents', 'problem_type'] = \"family\"\n",
        "data.loc[data['problem_type']=='Issues with Children', 'problem_type'] = \"family\"\n",
        "data.loc[data['problem_type']=='Issues with Parents', 'problem_type'] = \"family\"\n",
        "\n",
        "data.loc[data['problem_type']=='ongoing depression', 'problem_type'] = \"emotional\"\n",
        "data.loc[data['problem_type']=='Appearance Anxiety', 'problem_type'] = \"emotional\"\n",
        "\n",
        "data.loc[data['problem_type']=='Sleep Problems', 'problem_type'] = \"others\"\n",
        "data.loc[data['problem_type']=='Procrastination', 'problem_type'] = \"others\"\n",
        "data.loc[data['problem_type']=='Alcohol Abuse', 'problem_type'] = \"others\"\n",
        "\n",
        "display(data.problem_type.value_counts())\n",
        "\n",
        "labels = set(data.problem_type.values)\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdRDLlS6IQUT"
      },
      "outputs": [],
      "source": [
        "newdata = data[['situation', 'problem_type']]\n",
        "newdata.to_csv(\"newdata.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing \n",
        "y = data['problem_type']\n",
        "X = data['situation']\n",
        "length = data['situation'].apply(len)\n",
        "data = data.assign(Len_Situation=length)\n",
        "\n",
        "#Split train & test\n",
        "text_train, text_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
      ],
      "metadata": {
        "id": "Tn7sutj2TG9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eI_AXymbzYV",
        "outputId": "65fc4983-d00a-40bd-f61d-8fa9eba7c5e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1143    I made a mistake at work and I feel I might ge...\n",
              "1053    Husband of 9 years walked out on me and our ch...\n",
              "372     My boss recently left the company and I am not...\n",
              "826     my boss does not care about my and there is no...\n",
              "840                                          I was fired.\n",
              "                              ...                        \n",
              "715     I am always in the mood of depression. I thoug...\n",
              "905     thinking about what classes are best to take i...\n",
              "1096            I did not study and did poorly on a test.\n",
              "235     I don't have any friends. People keep betrayin...\n",
              "1061    My partner is abusing me for not earning money...\n",
              "Name: situation, Length: 910, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(text_train))"
      ],
      "metadata": {
        "id": "3k6T3QuttwbL",
        "outputId": "b61ca6ea-c009-421e-9d2a-b607dd4a9371",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ = pd.concat([text_train, y_train], axis=1)\n",
        "train_.to_csv(\"train_.csv\")\n",
        "test_ = pd.concat([text_test, y_test], axis=1)\n",
        "test_.to_csv(\"test_.csv\")"
      ],
      "metadata": {
        "id": "5ogUv4uKuhsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using BERT to do classification"
      ],
      "metadata": {
        "id": "YPJK2bOpSak0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry5YrWFkOnqP"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVYxzKoqOnqR",
        "outputId": "32fe09f2-4821-4f53-e622-c743ec11d964",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'have', 'a', 'bad', 'day']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.tokenize('I have a bad day')\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0IOAvUYOnqS"
      },
      "source": [
        "We can numericalize tokens using our vocabulary using `tokenizer.convert_tokens_to_ids`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XQ4sqbBOnqS",
        "outputId": "5c13dfcd-3a20-4516-a247-e279d1aa456f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1045, 2031, 1037, 2919, 2154]\n"
          ]
        }
      ],
      "source": [
        "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBF60ih8OnqT",
        "outputId": "e6151542-fed0-4801-e50a-58918cf6441d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ]
        }
      ],
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEVuFI1ZOnqT"
      },
      "source": [
        "We can get the indexes of the special tokens by converting them using the vocabulary..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzw--JnaOnqT",
        "outputId": "495f5bfd-1f69-4b94-c4a0-7360d6a77c9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 102 0 100\n"
          ]
        }
      ],
      "source": [
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZiNdP5JOnqU",
        "outputId": "141c1651-f874-447f-ecfd-cc8b5c1c2982",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        }
      ],
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWYCy_wvOnqV"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy.data import Field\n",
        "from torchtext.legacy.data import TabularDataset\n",
        "from torchtext.legacy.data import Iterator\n",
        "from torchtext.legacy.data import LabelField, BucketIterator"
      ],
      "metadata": {
        "id": "anizfStknuNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc9APA92OnqV"
      },
      "outputs": [],
      "source": [
        "TEXT = Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = LabelField()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4zPMYKhOnqW"
      },
      "source": [
        "We load the data and create the validation splits as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "293VdkW5OnqW"
      },
      "outputs": [],
      "source": [
        "from torchtext.legacy import datasets\n",
        "\n",
        "train_data, valid_data = TabularDataset.splits(\n",
        "            path='./',\n",
        "            format='csv',\n",
        "            train = 'train_.csv',\n",
        "            validation = 'test_.csv',\n",
        "            skip_header=True,\n",
        "            fields=[('ID',None),  ('text', TEXT), ('label', LABEL)],\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRdI8KHBOnqW",
        "outputId": "d5f95a74-5ae2-4ebf-ef28-055cabfb563b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 910\n",
            "Number of validation examples: 390\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = TabularDataset(\n",
        "            path='newdata.csv',\n",
        "            format='csv',\n",
        "            skip_header=True,\n",
        "            fields=[('ID',None),  ('text', TEXT), ('label', LABEL)],\n",
        "        )"
      ],
      "metadata": {
        "id": "banSHP-QxLiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwvtmqCVOnqW"
      },
      "source": [
        "We can check an example and ensure that the text has already been numericalized."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vars(test_data.examples[29]))"
      ],
      "metadata": {
        "id": "wB5oGxFKx_5H",
        "outputId": "d0d725de-3e93-404a-cdba-1924b5253785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': [2026, 2879, 2767, 2187, 2033, 2005, 2498, 1010, 1045, 2699, 2000, 3198, 2005, 7526, 2021, 1045, 2180, 1005, 1056, 2131, 2151, 2518, 1010, 2191, 2033, 14777, 1998, 2371, 2894, 1012], 'label': 'relationship'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABlUBmITOnqX",
        "outputId": "5e146446-9dac-4845-c34d-df2dc3f84085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': [1045, 3728, 3631, 2039, 2007, 2026, 6898, 1998, 1045, 1005, 1049, 2383, 1037, 2524, 2051, 2138, 2009, 1005, 1055, 2471, 1996, 11938, 1012], 'label': 'relationship'}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[6]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ0TtK0tOnqX"
      },
      "source": [
        "We can use the `convert_ids_to_tokens` to transform these indexes back into readable tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Tf4kdZXOnqX",
        "outputId": "33beadc5-eaf0-4f3f-907c-4ae7a5e8765b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'recently', 'broke', 'up', 'with', 'my', 'boyfriend', 'and', 'i', \"'\", 'm', 'having', 'a', 'hard', 'time', 'because', 'it', \"'\", 's', 'almost', 'the', 'holidays', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aLtx7I_OnqX"
      },
      "source": [
        "Although we've handled the vocabulary for the text, we still need to build the vocabulary for the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BYGrZhoOnqX"
      },
      "outputs": [],
      "source": [
        "LABEL.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGxN5yq6OnqX",
        "outputId": "faf44b38-3d7d-49f9-fc63-f58c6430f00a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(None, {'emotional': 0, 'work': 1, 'relationship': 2, 'friendship': 3, 'school': 4, 'others': 5, 'family': 6})\n"
          ]
        }
      ],
      "source": [
        "print(LABEL.vocab.stoi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Ce5VFyOnqX"
      },
      "source": [
        "As before, we create the iterators. Ideally we want to use the largest batch size that we can as I've found this gives the best results for transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTiZO_RTOnqY"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    sort=False,\n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amkr6A0ZOnqY"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "Next, we'll load the pre-trained model, making sure to load the same model as we did for the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiCrzGR3OnqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d833e18f-b925-484b-ee9f-3043d2c8dd32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GxdlbH0OnqY"
      },
      "source": [
        "Next, we'll define our actual model. \n",
        "\n",
        "Instead of using an embedding layer to get embeddings for our text, we'll be using the pre-trained transformer model. These embeddings will then be fed into a GRU to produce a prediction for the sentiment of the input sentence. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. The rest of the initialization is standard.\n",
        "\n",
        "Within the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. The transformer actually returns the embeddings for the whole sequence as well as a *pooled* output. The [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) states that the pooled output is \"usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\", hence we will not be using it. The rest of the forward pass is the standard implementation of a recurrent model, where we take the hidden state over the final time-step, and pass it through a linear layer to get our predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCcmrWJxOnqY"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        \n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "                \n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "        \n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqHT2eh6OnqZ"
      },
      "source": [
        "Next, we create an instance of our model using standard hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBg4quS_OnqZ"
      },
      "outputs": [],
      "source": [
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 7\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "\n",
        "model = BERTGRUSentiment(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WWZqPdHOnqZ"
      },
      "source": [
        "We can check how many parameters the model has. Our standard models have under 5M, but this one has 112M! Luckily, 110M of these parameters are from the transformer and we will not be training those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80GMS9zWOnqZ",
        "outputId": "9953978b-ed66-4dbd-f978-1c6f31ae2007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 112,244,487 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35ONPfw0OnqZ"
      },
      "source": [
        "In order to freeze paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9rsqEh2OnqZ"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpNsXNdNOnqZ"
      },
      "source": [
        "We can now see that our model has under 3M trainable parameters, making it almost comparable to the `FastText` model. However, the text still has to propagate through the transformer which causes training to take considerably longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46WAFh-BOnqZ",
        "outputId": "5011bc07-c174-42e0-9d8d-543b5fd1709a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 2,762,247 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DV6v9ZgOnqa"
      },
      "source": [
        "We can double check the names of the trainable parameters, ensuring they make sense. As we can see, they are all the parameters of the GRU (`rnn`) and the linear layer (`out`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKfvO_hWOnqa",
        "outputId": "c0a00ef5-7db4-4ed6-f4cb-90cdf26672cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnn.weight_ih_l0\n",
            "rnn.weight_hh_l0\n",
            "rnn.bias_ih_l0\n",
            "rnn.bias_hh_l0\n",
            "rnn.weight_ih_l0_reverse\n",
            "rnn.weight_hh_l0_reverse\n",
            "rnn.bias_ih_l0_reverse\n",
            "rnn.bias_hh_l0_reverse\n",
            "rnn.weight_ih_l1\n",
            "rnn.weight_hh_l1\n",
            "rnn.bias_ih_l1\n",
            "rnn.bias_hh_l1\n",
            "rnn.weight_ih_l1_reverse\n",
            "rnn.weight_hh_l1_reverse\n",
            "rnn.bias_ih_l1_reverse\n",
            "rnn.bias_hh_l1_reverse\n",
            "out.weight\n",
            "out.bias\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTttgiGxOnqa"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "As is standard, we define our optimizer and criterion (loss function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9yMeXY6Onqa"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfAlBtuiOnqa"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abAtHyOAOnqa"
      },
      "source": [
        "Place the model and criterion onto the GPU (if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HRdK4hdOnqa"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdo649rwOnqb"
      },
      "source": [
        "Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SSdtfDCOnqb"
      },
      "outputs": [],
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    # correct.to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "    return correct.sum() / len(correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIdA1HrqOnqb"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for batch in iterator:\n",
        "  \n",
        "          \n",
        "          predictions = model(batch.text).squeeze(1)\n",
        "          \n",
        "          loss = criterion(predictions, batch.label)\n",
        "          \n",
        "          acc = categorical_accuracy(predictions, batch.label)\n",
        "          \n",
        "\n",
        "          \n",
        "          epoch_loss += loss.item()\n",
        "          epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "W7aRjw9LVtHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSRGOsxVOnqb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvenCWA6Onqb"
      },
      "source": [
        "Finally, we'll train our model. This takes considerably longer than any of the previous models due to the size of the transformer. Even though we are not training any of the transformer's parameters we still need to pass the data through the model which takes a considerable amount of time on a standard GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oOmybBNOnqb",
        "outputId": "2ac19c33-9434-45c4-f3ba-8f67d7bd09ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 1.336 | Train Acc: 50.81%\n",
            "\t Val. Loss: 0.900 |  Val. Acc: 70.08%\n",
            "Epoch: 02 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.719 | Train Acc: 77.57%\n",
            "\t Val. Loss: 0.839 |  Val. Acc: 74.58%\n",
            "Epoch: 03 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.568 | Train Acc: 82.08%\n",
            "\t Val. Loss: 0.826 |  Val. Acc: 77.25%\n",
            "Epoch: 04 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.456 | Train Acc: 85.39%\n",
            "\t Val. Loss: 0.809 |  Val. Acc: 76.92%\n",
            "Epoch: 05 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.355 | Train Acc: 88.36%\n",
            "\t Val. Loss: 0.819 |  Val. Acc: 74.92%\n",
            "Epoch: 06 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.297 | Train Acc: 90.01%\n",
            "\t Val. Loss: 0.959 |  Val. Acc: 74.67%\n",
            "Epoch: 07 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.201 | Train Acc: 93.50%\n",
            "\t Val. Loss: 0.803 |  Val. Acc: 78.92%\n",
            "Epoch: 08 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.157 | Train Acc: 93.92%\n",
            "\t Val. Loss: 0.938 |  Val. Acc: 76.75%\n",
            "Epoch: 09 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.123 | Train Acc: 95.38%\n",
            "\t Val. Loss: 1.074 |  Val. Acc: 75.08%\n",
            "Epoch: 10 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.119 | Train Acc: 95.71%\n",
            "\t Val. Loss: 1.026 |  Val. Acc: 77.42%\n",
            "Epoch: 11 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.075 | Train Acc: 97.70%\n",
            "\t Val. Loss: 1.083 |  Val. Acc: 75.92%\n",
            "Epoch: 12 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.082 | Train Acc: 97.02%\n",
            "\t Val. Loss: 1.255 |  Val. Acc: 74.08%\n",
            "Epoch: 13 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.098 | Train Acc: 95.82%\n",
            "\t Val. Loss: 1.243 |  Val. Acc: 75.92%\n",
            "Epoch: 14 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.067 | Train Acc: 97.68%\n",
            "\t Val. Loss: 1.194 |  Val. Acc: 74.33%\n",
            "Epoch: 15 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.043 | Train Acc: 98.79%\n",
            "\t Val. Loss: 1.307 |  Val. Acc: 76.92%\n",
            "Epoch: 16 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.055 | Train Acc: 98.25%\n",
            "\t Val. Loss: 1.352 |  Val. Acc: 74.92%\n",
            "Epoch: 17 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.025 | Train Acc: 99.11%\n",
            "\t Val. Loss: 1.225 |  Val. Acc: 79.67%\n",
            "Epoch: 18 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.047 | Train Acc: 98.14%\n",
            "\t Val. Loss: 1.320 |  Val. Acc: 76.00%\n",
            "Epoch: 19 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.031 | Train Acc: 99.01%\n",
            "\t Val. Loss: 1.280 |  Val. Acc: 77.25%\n",
            "Epoch: 20 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.025 | Train Acc: 99.01%\n",
            "\t Val. Loss: 1.256 |  Val. Acc: 78.75%\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "loss_x = []\n",
        "loss_y = []\n",
        "acc_x = []\n",
        "acc_y = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    loss_x.append(int(train_loss * 1000) / 1000)\n",
        "    acc_x.append(int(train_acc * 1000) / 1000)\n",
        "\n",
        "\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    loss_y.append(int(valid_loss * 1000) / 1000)\n",
        "    acc_y.append(int(valid_acc * 1000) / 1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olA5_qnYGjLF",
        "outputId": "cbdef0df-5a10-4ebe-fe1a-979f5c895bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.899,\n",
              " 0.839,\n",
              " 0.825,\n",
              " 0.809,\n",
              " 0.818,\n",
              " 0.958,\n",
              " 0.803,\n",
              " 0.937,\n",
              " 1.073,\n",
              " 1.026,\n",
              " 1.083,\n",
              " 1.254,\n",
              " 1.243,\n",
              " 1.193,\n",
              " 1.307,\n",
              " 1.352,\n",
              " 1.225,\n",
              " 1.32,\n",
              " 1.28,\n",
              " 1.255]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "     \n",
        "plt.figure(figsize=(8,6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss_x,label=\"train\")\n",
        "plt.plot(loss_y,label=\"validation\")\n",
        "plt.title('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('')\n",
        "plt.grid(loss_x)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(acc_x,label=\"train\")\n",
        "plt.plot(acc_y,label=\"validation\")\n",
        "plt.title('acc')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('')\n",
        "plt.grid(acc_x)\n",
        "\n",
        "plt.suptitle(\"Train-Validation loss and accuracy\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "15y--RRsBYZT",
        "outputId": "6e686966-a757-4bec-818b-d564053021e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR7NFcQFOnqc"
      },
      "source": [
        "We'll load up the parameters that gave us the best validation loss and try these on the test set - which gives us our best results so far!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA2GsNkVOnqc",
        "outputId": "4cedcb39-86a7-4b5b-c92f-02b05346ff16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.305 | Test Acc: 91.69%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5NSos47Onqc"
      },
      "source": [
        "## Inference\n",
        "\n",
        "We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories=['emotional','work','relationship','friendship','school','others','family']"
      ],
      "metadata": {
        "id": "Ef_X-xmqZIiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBtmX4HqOnqc"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    index = torch.argmax(prediction)\n",
        "    label = categories[index.cpu().numpy()]\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKkHOytROnqc",
        "outputId": "f7a5ee2d-66fd-4f83-aa1b-1f8b28613c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'others'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "predict_sentiment(model, tokenizer, \"I have totally a bad day\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SF5z4LAbQOX",
        "outputId": "04c9cba1-a1d5-45d9-98bf-87e794400a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1143            work\n",
              "1053    relationship\n",
              "372             work\n",
              "826             work\n",
              "840        emotional\n",
              "776        emotional\n",
              "778     relationship\n",
              "1076    relationship\n",
              "887        emotional\n",
              "1125       emotional\n",
              "Name: problem_type, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_train[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo913FSubCqC",
        "outputId": "ea80eab5-b992-4843-f8f5-839820cabad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1143    I made a mistake at work and I feel I might ge...\n",
              "1053    Husband of 9 years walked out on me and our ch...\n",
              "372     My boss recently left the company and I am not...\n",
              "826     my boss does not care about my and there is no...\n",
              "840                                          I was fired.\n",
              "776     when i lost my job and i was at home for long ...\n",
              "778     I recently broke up with my boyfriend and I'm ...\n",
              "1076    My ex who i loved so much and almost gave her ...\n",
              "887     I have been in a depression since my father di...\n",
              "1125    I am sad about not having family near me. I am...\n",
              "Name: situation, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "situation = []\n",
        "i=0\n",
        "for t in text_train[:10]:\n",
        "  situation.append(predict_sentiment(model, tokenizer, t))\n",
        "  \n",
        "situation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnIuKvUYZ4VY",
        "outputId": "110feed7-74f0-47a7-8131-9c6846e77fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['work',\n",
              " 'relationship',\n",
              " 'work',\n",
              " 'work',\n",
              " 'work',\n",
              " 'work',\n",
              " 'relationship',\n",
              " 'relationship',\n",
              " 'emotional',\n",
              " 'emotional']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Inference"
      ],
      "metadata": {
        "id": "BmxcVwyad36c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKTfH8NLfq9C",
        "outputId": "2e3e8397-acef-4e80-ef4f-14ea2dd42e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        \n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "                \n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "        \n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "kZgPe2ECeFJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IkL40V6emXq"
      },
      "source": [
        "Next, we create an instance of our model using standard hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_wdKWMzemXr"
      },
      "outputs": [],
      "source": [
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 7\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "max_input_length = 512\n",
        "\n",
        "model = BERTGRUSentiment(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"tut6-model.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWrmN3EcetsL",
        "outputId": "eb79cb57-85f6-47dd-bcfe-90ae4b68cc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMzVwrUqemXs"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfbwUtCemXs"
      },
      "source": [
        "Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcJG19OremXs"
      },
      "outputs": [],
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    # correct.to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "    return correct.sum() / len(correct)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for batch in iterator:\n",
        "  \n",
        "          \n",
        "          predictions = model(batch.text).squeeze(1)\n",
        "          \n",
        "          loss = criterion(predictions, batch.label)\n",
        "          \n",
        "          acc = categorical_accuracy(predictions, batch.label)\n",
        "          \n",
        "\n",
        "          \n",
        "          epoch_loss += loss.item()\n",
        "          epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "JoCnOj2KemXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGPgj_9JemXt"
      },
      "source": [
        "## Inference\n",
        "\n",
        "We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories=['emotional','work','relationship','friendship','school','others','family']"
      ],
      "metadata": {
        "id": "pMm3woiYemXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Eif4e_kemXt"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    index = torch.argmax(prediction)\n",
        "    label = categories[index.cpu().numpy()]\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "1b4ae09c-3f36-4e5c-d752-81319e22341a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w2NhXvbGemXt"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'others'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "predict_sentiment(model, tokenizer, \"I have totally a bad day\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Train_BERT",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}